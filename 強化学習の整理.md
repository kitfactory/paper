# 触って簡単！強化学習！

## 強化学習とは？

強化学習ですが、ことです。

## マルコフ決定過程

以下の４要素。

State: 状態
Action:行動
Reward:報酬（即時）
Transition:遷移関数


MDPの報酬=即時報酬を割り引いて合計したもの。


## モデルベース/モデルフリー

モデルベース：遷移関数/即時報酬を持つ
モデルフリー：遷移関数/即時報酬を持たない

## ベルマン方程式

価値を再帰的かつ期待値で表した方程式


## ポリシーベース/バリューベース

ポリシーベース：方策を探索して高い価値を得る
バリューベース：価値を直接高くなるようにする。


## 経験の蓄積と活用

経験（もっとも良い結果）を採択し続けると、新しい手法を探索できない。
そのため、新しい手法を探索して蓄積する必要がある。

* Epsilon-Greedy法：ある確率で探索する。
* モンテカルロ法：ランダムで幾通りか試した上で判断
* TD法：何ステップか進めてみた結果で判断
    * QラーニングをQ値：行動価値関数を学ぶ学習、TD法の一つ
* Multi-Step:
    Rainbow, A3C/A2C, DDPGなどはモンテカルロ法とTD法の中間的手法


## オンポリシー、オフポリシー
* 方策があるかないかをオンポリシー、オフポリシー
    * Valueベースでも方策がある：オンポリシー、SARSA
    * Valueベースで方策がない：オフポリシー、Q-学習

* Actor Critic法
    * Actor : 方策、 Critic : 状態判断を相互に学習する。
    * オンポリシーとオフポリシーの中間手法


## いくつかの実装
* Rainbow
* A2C/A3C


## 強化学習の問題

* サンプル効率が悪い
* 局所解に陥りやすい、過学習しやすい
* 再現性が低い


## 強化学習の弱点の克服

* サンプル効率の悪さ　→　モデルベースとの併用, Dynaなど。
* 学習の仕方を学習する Learning to Learn
* 再現性の悪さ　→　進化戦略を取り入れる
* 局所解に落ちる　→　模倣学習させてある程度の解に。示された手本から報酬関数を逆算する、逆強化学習。


## 報酬



## Agent

## Environment

## 
