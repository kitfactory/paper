# 動かして学ぼう。簡単、強化学習！

## 強化学習とは？

強化学習は、囲碁のプログラム、Alpha GOや、

[Google Deepmind、人工知能『DQN』を開発。レトロゲームを自力で学習、人間に勝利](https://japanese.engadget.com/2015/02/26/google-deepmind-dqn/)で話題となった学習方法の一つです。

エージェントが環境の中で行動を繰り返しながら学習していくことが特徴的です。理論的には、うーんと思うような、難しい用語が出てくるわけですが、昨今ではプログラムのライブラリやフレームワークが充実しています。色々な論文を考える前に、まずは触ってみるのが、一番わかりやすいと思います。

## 強化学習の枠組み（テーマ図）

強化学習のテーマ図は、こちらの図です。

エージェントが、環境に対して行動し、それに対して報償を得る、このループを通じて試行錯誤で学習します。それでは、

## インストール

>pip install tensorflow
>pip install tf-agents


Windowsの環境の人は、次のステップのgymのインストールで、box2d-pyのインストールがうまくいかないと怒られることになります。事前に、こちらからWheelファイルを用意したので、インストールしておいてください。(python 3.7しか用意してません、すみません。)

>pip install xxxxxxxxx.whl


>pip install gym
>pip install gym[all]

mujocoのエラーは無視していただいて構いません。mujocoは商用ライセンスのある有料ソフトです。

## フレームワークの仕組みを覚えよう

強化学習のフレームワークでは、以下のような構成要素があります。

### Environment

環境のことです。取りうる状態、取りうるアクション、報酬の状態を取得することができます。また画像を描画することもできます。

### Agent

強化学習のアルゴリズム部分です。どのようにPolicyを学習していくかについて、さまざまなアルゴリズムで実装されています。エージェントには、どのようなニューラルネットで学習するかを設定します。

エージェントには環境から取得する環境のデータや報酬などのデータからサイズを決めたニューラルネットを設定します。

### Policy

状態に対して、どうアクションをすれば良いかの方策です。学習を通じて、この方策を学ぶのが強化学習の目的です。

### Replay Buffer

再現をするためのバッファです。学習に使用するデータを保持するために用います。リプレイバッファでは各試行（エピソード）での遷移を蓄積しておきます。これを一定程度ためて、それらのエピソードをまとめて評価して、学習していきます。どれくらいの量のデータが必要かは、エージェントから取得します。

## 実装

これらの役割を理解したところで、早速実装していきましょう。

### 環境の準備

学習用と評価用の二つの環境を用意します。通常、OpenAI Gymを使用するのですが、TF-AgentsではTFPyEnvironmentへラップして使用します。

```
train_py_env = suite_gym.load(env_name)
eval_py_env = suite_gym.load(env_name)

train_env = tf_py_environment.TFPyEnvironment(train_py_env)
eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)
```

### エージェントの準備

エージェント(アルゴリズム)にはREINFORCEを使ってみます。REINFORCEには、ReinforceAgentを使います。より高度な学習もできますが、カスタマイズしたネットワークを学習させることもできますが、ActorDistributionNetworkを使用しましょう。

```
actor_net = actor_distribution_network.ActorDistributionNetwork(
    train_env.observation_spec(),
    train_env.action_spec(),
    fc_layer_params=fc_layer_params)


optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)

train_step_counter = tf.compat.v2.Variable(0)

tf_agent = reinforce_agent.ReinforceAgent(
    train_env.time_step_spec(),
    train_env.action_spec(),
    actor_network=actor_net,
    optimizer=optimizer,
    normalize_returns=True,
    train_step_counter=train_step_counter)
tf_agent.initialize()

```

### 評価方法の準備

次に評価関数を準備します。

```
def compute_avg_return(environment, policy, num_episodes=10):

  total_return = 0.0
  for _ in range(num_episodes):

    time_step = environment.reset()
    episode_return = 0.0

    while not time_step.is_last():
      action_step = policy.action(time_step)
      time_step = environment.step(action_step.action)
      episode_return += time_step.reward
    total_return += episode_return

  avg_return = total_return / num_episodes
  return avg_return.numpy()[0]
```


## データの蓄積

```python
replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
    data_spec=tf_agent.collect_data_spec,
    batch_size=train_env.batch_size,
    max_length=replay_buffer_capacity)


def collect_episode(environment, policy, num_episodes):

  episode_counter = 0
  environment.reset()

  while episode_counter < num_episodes:
    time_step = environment.current_time_step()
    action_step = policy.action(time_step)
    next_time_step = environment.step(action_step.action)
    traj = trajectory.from_transition(time_step, action_step, next_time_step)

    # Add trajectory to the replay buffer
    replay_buffer.add_batch(traj)

    if traj.is_boundary():
      episode_counter += 1

```








### Data Collection



これらの要素を使って学習をします。




### Policy

方策　\(pi\)

## Q関数

\[Q(a,s)\]とは状態とアクションから報酬を最大化する関数を求めること。

## 

